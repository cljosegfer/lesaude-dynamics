{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1551485c",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3daa6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import ast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7537b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_CSV = '/home/remote/Documents/datasets/lesaude/mimic-iv-ecg-ext-icd-diagnostic-labels-for-mimic-iv-ecg-1.0.1/records_w_diag_icd10.csv'\n",
    "WAVEFORM_H5 = '/home/remote/Documents/datasets/lesaude/mimic-iv-ecg-monolith/mimic_iv_ecg_waveforms.h5'\n",
    "LABEL_H5 = '/home/remote/Documents/datasets/lesaude/mimic-iv-ecg-monolith/mimic_iv_ecg_icd.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd57a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CHAPTER = 'I'  # Circulatory System\n",
    "CODE_LENGTH = 3       # \"I\" + 2 digits\n",
    "BATCH_SIZE = 1000     # Buffer size for writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6e507",
   "metadata": {},
   "source": [
    "# monolith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ea1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_codes(code_list_str):\n",
    "    if pd.isna(code_list_str) or code_list_str == '':\n",
    "        return set()\n",
    "    try:\n",
    "        raw_codes = ast.literal_eval(code_list_str)\n",
    "    except:\n",
    "        return set()\n",
    "    clean_codes = set()\n",
    "    for code in raw_codes:\n",
    "        if code.startswith(TARGET_CHAPTER):\n",
    "            clean_codes.add(code[:CODE_LENGTH])\n",
    "    return clean_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52434559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(WAVEFORM_H5, 'r') as f:\n",
    "#     # We read the IDs and Timestamps that actually exist in the compiled dataset\n",
    "#     h5_subjects = f['subject_id'][:]\n",
    "#     h5_studies = f['study_id'][:]\n",
    "#     h5_timestamps = f['ecg_time'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(LABELS_CSV)\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8dc26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels['parsed_set'] = df_labels['all_diag_all'].apply(preprocess_codes)\n",
    "all_codes = set()\n",
    "for code_set in df_labels['parsed_set']:\n",
    "    all_codes.update(code_set)\n",
    "\n",
    "vocab_list = sorted(list(all_codes))\n",
    "vocab_map = {code: i for i, code in enumerate(vocab_list)}\n",
    "vocab_size = len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "230f3826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing: 100%|██████████| 800035/800035 [00:27<00:00, 29551.62it/s]\n"
     ]
    }
   ],
   "source": [
    "label_lookup = {}\n",
    "for _, row in tqdm(df_labels.iterrows(), total=len(df_labels), desc=\"Hashing\"):\n",
    "    subj_id = int(row['subject_id'])\n",
    "    # Handle NaN study_ids (some records might miss them)\n",
    "    study_id = int(row['study_id']) if not pd.isna(row['study_id']) else 0\n",
    "    \n",
    "    vector = np.zeros(vocab_size, dtype='int8')\n",
    "    for code in row['parsed_set']:\n",
    "        if code in vocab_map:\n",
    "            vector[vocab_map[code]] = 1\n",
    "    label_lookup[(subj_id, study_id)] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352b463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 800035/800035 [00:04<00:00, 173569.65it/s]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(WAVEFORM_H5, 'r') as f_src, \\\n",
    "        h5py.File(LABEL_H5, 'w') as f_dst:\n",
    "    \n",
    "    total_records = f_src['subject_id'].shape[0]\n",
    "    \n",
    "    # Create Datasets\n",
    "    dset_labels = f_dst.create_dataset('icd', (total_records, vocab_size), dtype='i1')\n",
    "    dset_subj = f_dst.create_dataset('subject_id', (total_records,), dtype='i4')\n",
    "    dset_study = f_dst.create_dataset('study_id', (total_records,), dtype='i4')\n",
    "    \n",
    "    # Save Vocabulary for reference\n",
    "    dt_str = h5py.special_dtype(vlen=str)\n",
    "    dset_vocab = f_dst.create_dataset('vocabulary', (vocab_size,), dtype=dt_str)\n",
    "    dset_vocab[:] = vocab_list\n",
    "    \n",
    "    # Buffers\n",
    "    buf_labels = np.zeros((BATCH_SIZE, vocab_size), dtype='int8')\n",
    "    buf_subj = np.zeros(BATCH_SIZE, dtype='int32')\n",
    "    buf_study = np.zeros(BATCH_SIZE, dtype='int32')\n",
    "    \n",
    "    buf_ptr = 0\n",
    "    global_ptr = 0\n",
    "    match_count = 0\n",
    "    \n",
    "    # Source Datasets\n",
    "    src_subjs = f_src['subject_id']\n",
    "    src_studies = f_src['study_id']\n",
    "    \n",
    "    for i in tqdm(range(total_records), desc=\"Processing\"):\n",
    "        # Read IDs from source (Preserving Order is Key)\n",
    "        s_id = src_subjs[i]\n",
    "        st_id = src_studies[i]\n",
    "        \n",
    "        # Lookup Labels\n",
    "        key = (s_id, st_id)\n",
    "        if key in label_lookup:\n",
    "            vector = label_lookup[key]\n",
    "            match_count += 1\n",
    "        else:\n",
    "            vector = np.zeros(vocab_size, dtype='int8')\n",
    "        \n",
    "        # Fill Buffers\n",
    "        buf_labels[buf_ptr] = vector\n",
    "        buf_subj[buf_ptr] = s_id\n",
    "        buf_study[buf_ptr] = st_id\n",
    "        \n",
    "        buf_ptr += 1\n",
    "        \n",
    "        # Flush if full\n",
    "        if buf_ptr >= BATCH_SIZE:\n",
    "            end_ptr = global_ptr + BATCH_SIZE\n",
    "            dset_labels[global_ptr:end_ptr] = buf_labels\n",
    "            dset_subj[global_ptr:end_ptr] = buf_subj\n",
    "            dset_study[global_ptr:end_ptr] = buf_study\n",
    "            \n",
    "            global_ptr += BATCH_SIZE\n",
    "            buf_ptr = 0\n",
    "    \n",
    "    # Flush remaining\n",
    "    if buf_ptr > 0:\n",
    "        end_ptr = global_ptr + buf_ptr\n",
    "        dset_labels[global_ptr:end_ptr] = buf_labels[:buf_ptr]\n",
    "        dset_subj[global_ptr:end_ptr] = buf_subj[:buf_ptr]\n",
    "        dset_study[global_ptr:end_ptr] = buf_study[:buf_ptr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4fd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
